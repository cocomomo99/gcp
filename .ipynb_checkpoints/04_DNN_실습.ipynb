{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "* DNN(Deep Neural Net) 실습과제\n",
    "\n",
    "[1] Boston 주택 가격 회귀 예측 모델을 텐서플로로  3층 신경망을 구현하여 RMSE를 구하여 단층 신경망과 비교하여 보세요\n",
    "    단층일 때보다 RMSE가 낮아지도록 구현한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "         CRIM    ZN  INDUS    NOX     RM   AGE     DIS  TAX  PTRATIO  MEDV\n0     2.30040   0.0  19.58  0.605  6.319  96.1  2.1000  403     14.7  23.8\n1    13.35980   0.0  18.10  0.693  5.887  94.7  1.7821  666     20.2  12.7\n2     0.12744   0.0   6.91  0.448  6.770   2.9  5.7209  233     17.9  26.6\n3     0.15876   0.0  10.81  0.413  5.961  17.5  5.2873  305     19.2  21.7\n4     0.03768  80.0   1.52  0.404  7.274  38.3  7.3090  329     12.6  34.6\n..        ...   ...    ...    ...    ...   ...     ...  ...      ...   ...\n395   0.23912   0.0   9.69  0.585  6.019  65.3  2.4091  391     19.2  21.2\n396   0.04560   0.0  13.89  0.550  5.888  56.0  3.1121  276     16.4  23.3\n397   1.38799   0.0   8.14  0.538  5.950  82.0  3.9900  307     21.0  13.2\n398   7.36711   0.0  18.10  0.679  6.193  78.1  1.9356  666     20.2  11.0\n399   0.14150   0.0   6.91  0.448  6.169   6.6  5.7209  233     17.9  25.3\n\n[400 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.30040</td>\n      <td>0.0</td>\n      <td>19.58</td>\n      <td>0.605</td>\n      <td>6.319</td>\n      <td>96.1</td>\n      <td>2.1000</td>\n      <td>403</td>\n      <td>14.7</td>\n      <td>23.8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.35980</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.693</td>\n      <td>5.887</td>\n      <td>94.7</td>\n      <td>1.7821</td>\n      <td>666</td>\n      <td>20.2</td>\n      <td>12.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.12744</td>\n      <td>0.0</td>\n      <td>6.91</td>\n      <td>0.448</td>\n      <td>6.770</td>\n      <td>2.9</td>\n      <td>5.7209</td>\n      <td>233</td>\n      <td>17.9</td>\n      <td>26.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15876</td>\n      <td>0.0</td>\n      <td>10.81</td>\n      <td>0.413</td>\n      <td>5.961</td>\n      <td>17.5</td>\n      <td>5.2873</td>\n      <td>305</td>\n      <td>19.2</td>\n      <td>21.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.03768</td>\n      <td>80.0</td>\n      <td>1.52</td>\n      <td>0.404</td>\n      <td>7.274</td>\n      <td>38.3</td>\n      <td>7.3090</td>\n      <td>329</td>\n      <td>12.6</td>\n      <td>34.6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>0.23912</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0.585</td>\n      <td>6.019</td>\n      <td>65.3</td>\n      <td>2.4091</td>\n      <td>391</td>\n      <td>19.2</td>\n      <td>21.2</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>0.04560</td>\n      <td>0.0</td>\n      <td>13.89</td>\n      <td>0.550</td>\n      <td>5.888</td>\n      <td>56.0</td>\n      <td>3.1121</td>\n      <td>276</td>\n      <td>16.4</td>\n      <td>23.3</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>1.38799</td>\n      <td>0.0</td>\n      <td>8.14</td>\n      <td>0.538</td>\n      <td>5.950</td>\n      <td>82.0</td>\n      <td>3.9900</td>\n      <td>307</td>\n      <td>21.0</td>\n      <td>13.2</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>7.36711</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.679</td>\n      <td>6.193</td>\n      <td>78.1</td>\n      <td>1.9356</td>\n      <td>666</td>\n      <td>20.2</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>0.14150</td>\n      <td>0.0</td>\n      <td>6.91</td>\n      <td>0.448</td>\n      <td>6.169</td>\n      <td>6.6</td>\n      <td>5.7209</td>\n      <td>233</td>\n      <td>17.9</td>\n      <td>25.3</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/HOME/PycharmProjects/ai/.ipynb_checkpoints/data_set/boston_train.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('C:/Users/HOME/PycharmProjects/ai/.ipynb_checkpoints/data_set/boston_test.csv', dtype=float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[2.30040e+00, 0.00000e+00, 1.95800e+01, ..., 2.10000e+00,\n         4.03000e+02, 1.47000e+01],\n        [1.33598e+01, 0.00000e+00, 1.81000e+01, ..., 1.78210e+00,\n         6.66000e+02, 2.02000e+01],\n        [1.27440e-01, 0.00000e+00, 6.91000e+00, ..., 5.72090e+00,\n         2.33000e+02, 1.79000e+01],\n        ...,\n        [1.38799e+00, 0.00000e+00, 8.14000e+00, ..., 3.99000e+00,\n         3.07000e+02, 2.10000e+01],\n        [7.36711e+00, 0.00000e+00, 1.81000e+01, ..., 1.93560e+00,\n         6.66000e+02, 2.02000e+01],\n        [1.41500e-01, 0.00000e+00, 6.91000e+00, ..., 5.72090e+00,\n         2.33000e+02, 1.79000e+01]]),\n array([23.8, 12.7, 26.6, 21.7, 34.6, 35.4, 18.6, 33.1, 16.4, 15.2,  5. ,\n        28.4, 25. , 30.7, 15.6, 41.3, 24.2, 23.2, 21.9, 26.6, 23.4, 24.7,\n        14.9, 50. , 18.4, 21.8, 19.3, 27. , 13.3, 11.7, 23.6, 22. , 13.8,\n        31.7, 25. , 13.9, 50. , 46.7, 14.1, 17.4, 18.5,  8.5, 18.1, 19.2,\n        23.8, 37.6, 20. , 13.3, 27.9, 20.3, 19.6, 19.6, 10.5, 18.8, 12.3,\n        23. , 26.2, 13.6, 24.1, 19.2, 14.6, 29.8, 20.1, 20.7, 18.4, 21.4,\n        37.9, 19.3, 11.3, 32. , 50. , 12.8, 17.6, 14.9, 22.2, 15. , 20.2,\n        34.7, 22.6, 22.2, 20. , 15.3, 24.3, 37.3, 32.2, 23.1, 23. ,  7.2,\n        27.5, 50. , 24.6,  8.7, 50. , 13.4, 15. , 20.3, 22.5, 24.8,  8.1,\n        11.8, 22.9, 36.5, 23.7, 48.3, 50. , 25. , 24.7, 14.1, 50. , 23.1,\n        23.1, 50. , 23.9, 12.5, 23.9, 23. , 29.1, 32.4, 12.7, 14.8, 23.2,\n        25. ,  9.7, 16.5, 24.4, 13.8, 25. , 21.7, 22.9, 23.9, 22.8, 43.8,\n        20.6, 10.4, 20.8, 16.8, 10.9, 18. , 31.2, 39.8, 22. , 19.8, 18.7,\n        50. ,  8.8, 19.4, 19.4, 24.3, 21.9, 22.9, 42.3, 20.6, 20.5, 48.8,\n        18.2, 21. , 17.8, 27.5, 22.4,  8.3, 33.4, 15.2,  6.3, 13.5, 48.5,\n        31.1, 36.2,  7. , 27.9, 13.9, 37.2, 12.6, 28.2, 16.2, 16.1,  7.2,\n        17.8, 28.4, 33.8,  7.2, 16.6, 28.1, 19.4, 16.6, 19.5, 27.5, 14.1,\n        23.1, 17.2, 23.2, 10.9, 31.5, 19.6, 10.2, 22. , 24.8, 43.1, 28.7,\n        30.1, 19.1, 21.6, 20.4, 19.6, 22.9, 21.5, 21.1, 24.5, 31. , 21.7,\n         5.6, 30.1, 50. , 29.4, 23.7, 20.1,  8.3, 13.4, 18.7,  9.6, 18.2,\n        23.3, 17.2, 13.5, 12. , 14.5, 13.8, 20.5, 23.9, 20.5, 23.1, 29.8,\n        21.7, 25.2, 15. ,  7. , 22.7, 21.1, 13.1, 23.2, 22.8, 10.5, 18.7,\n        28.6, 22.7, 17.8, 21.4, 13.6, 17.5, 13.1, 29.6, 14.4, 19. , 23.4,\n        20.3, 16.7, 19.1, 22.6, 29. , 16.3, 23.7, 50. , 16.7, 27.1, 19. ,\n        19.9, 46. , 13.4, 22.2, 22. ,  7.5, 19.9, 23.7, 13.1, 32.7, 23.6,\n        25. , 19.8, 29.1, 28. , 12.7, 10.4, 18.3, 13.8, 20.3, 17.8, 18.8,\n        22.5, 35.2, 19.7, 26.6, 26.4, 35.4, 21.2, 50. , 23.3, 17.3, 13.3,\n        50. , 17.1, 17.1, 24.4, 18.6, 12.1, 14.4, 21.2, 25.1, 17.4, 16. ,\n        22.3, 11.9, 19.4, 31.6, 19.5, 23.1, 14.3, 13.1, 19.4, 17.8, 19.7,\n        22.5, 50. , 20.6, 17.1, 22.6, 20.6, 27.5, 36. ,  8.5, 18.9, 19.3,\n        29.6, 44.8, 15.1, 11.9, 38.7, 33. , 21.4, 50. , 25. , 21.8, 30.8,\n        20. , 30.5, 24. , 11.8, 22.2, 20.6, 14.5, 16.2, 23. , 28.5, 11.7,\n        21.2,  5. , 17.7, 35.1, 22.6, 18.5, 18.5, 17.2, 20.1, 24.5, 20.4,\n        30.3, 15.2, 19.3, 15.7, 21.9, 34.9, 24.7, 22.3, 33.1, 24.6, 19.5,\n        21.5, 24. , 14.3, 14.5, 44. , 29. , 23.8, 34.9, 22. , 32.9, 41.7,\n         8.8, 17.9, 21.4, 24.1, 45.4, 20.1, 15.6, 13.8, 15.6, 23.3, 21.2,\n        23.3, 13.2, 11. , 25.3]))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "\n",
    "x_test = df1.iloc[:,:-1].to_numpy()\n",
    "y_test = df1.iloc[:, -1].to_numpy()\n",
    "x_train , y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([9,5], dtype=tf.float64, name='weight1'))\n",
    "b1 = tf.Variable(tf.random.normal([5], dtype=tf.float64, name='bias1'))\n",
    "\n",
    "w2 = tf.Variable(tf.random.normal([5,2], dtype=tf.float64, name='weight2'))\n",
    "b2 = tf.Variable(tf.random.normal([2], dtype=tf.float64, name='bias2'))\n",
    "\n",
    "w3 = tf.Variable(tf.random.normal([2,1], dtype=tf.float64, name='weight3'))\n",
    "b3 = tf.Variable(tf.random.normal([1], dtype=tf.float64, name='bias3'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[2] Caesarian 제왕절개 Logistic Regression 모델을 텐서플로를 사용하여 3층으로신경망을 구현하여 Accuracy를 구하여 단층 신경망과 비교하여 보세요\n",
    "    단층일 때보다 정확도가 높아지도록 구현한다\n",
    "\n",
    "\n",
    "[힌트]  아래와 같이 구현하고 빈 칸을 완성하여 3층 신경망을 만든다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def layer1(X):\n",
    "    return  tf.nn.relu(tf.matmul(tf.Variable(X, dtype=tf.float64),w1) + b1 )\n",
    "\n",
    "def layer2(X):\n",
    "    return  tf.nn.relu(tf.matmul(layer1(X),w2) + b2 )\n",
    "\n",
    "def hypothesis(X):\n",
    "    return  tf.matmul(layer2(X),w3) + b3\n",
    "\n",
    "def cost():\n",
    "    h = hypothesis(x_train)\n",
    "    cost = tf.reduce_mean(tf.square(h - y_train))\n",
    "    return cost\n",
    "\n",
    "def pred(x):\n",
    "    return hypothesis(x)\n",
    "\n",
    "\n",
    "# 정확도 측정 : RMSE(Root Mean Squared Error)\n",
    "def get_rmse(y_test,preds):\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (preds[k] - y_test[k])**2\n",
    "    mse = squared_error/len(y_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float64, numpy=503.5653097426735>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 \n",
      " cost : 467.35231917114044 \n",
      " rmse : 21.733510680215982\n",
      "0100 \n",
      " cost : 102.7729110192687 \n",
      " rmse : 10.602766696571402\n",
      "0200 \n",
      " cost : 93.86769400931078 \n",
      " rmse : 10.066528949200706\n",
      "0300 \n",
      " cost : 93.26289977898097 \n",
      " rmse : 9.996819056622018\n",
      "0400 \n",
      " cost : 92.97591529823917 \n",
      " rmse : 9.927424779261843\n",
      "0500 \n",
      " cost : 92.74120106940359 \n",
      " rmse : 9.843079060951098\n",
      "0600 \n",
      " cost : 92.53924768921772 \n",
      " rmse : 9.74890922887652\n",
      "0700 \n",
      " cost : 92.37672697188734 \n",
      " rmse : 9.655553092633875\n",
      "0800 \n",
      " cost : 92.25954478118598 \n",
      " rmse : 9.570747281861932\n",
      "0900 \n",
      " cost : 92.18316875008021 \n",
      " rmse : 9.505712635995604\n",
      "1000 \n",
      " cost : 92.1328860330454 \n",
      " rmse : 9.463441298853542\n",
      "1100 \n",
      " cost : 92.0936319658125 \n",
      " rmse : 9.442155507568602\n",
      "1200 \n",
      " cost : 92.05704895386023 \n",
      " rmse : 9.43255952371657\n",
      "1300 \n",
      " cost : 92.02180647154775 \n",
      " rmse : 9.43058083790816\n",
      "1400 \n",
      " cost : 91.98601107538758 \n",
      " rmse : 9.431377385293601\n",
      "1500 \n",
      " cost : 91.95213500746092 \n",
      " rmse : 9.431732450684505\n",
      "1600 \n",
      " cost : 91.93223349621336 \n",
      " rmse : 9.435431243365299\n",
      "1700 \n",
      " cost : 91.92432583135101 \n",
      " rmse : 9.43952480611702\n",
      "1800 \n",
      " cost : 91.8530236282392 \n",
      " rmse : 9.450465684469407\n",
      "1900 \n",
      " cost : 91.8143452408369 \n",
      " rmse : 9.450476981191915\n",
      "2000 \n",
      " cost : 91.78871741951961 \n",
      " rmse : 9.461077526596007\n",
      "2100 \n",
      " cost : 91.75463952219381 \n",
      " rmse : 9.461745242316056\n",
      "2200 \n",
      " cost : 91.73045887081842 \n",
      " rmse : 9.47103042566039\n",
      "2300 \n",
      " cost : 91.75722925268768 \n",
      " rmse : 9.466352309878639\n",
      "2400 \n",
      " cost : 91.68338427356302 \n",
      " rmse : 9.47843267899813\n",
      "2500 \n",
      " cost : 91.66330579496137 \n",
      " rmse : 9.486333812010045\n",
      "2600 \n",
      " cost : 91.64657957613586 \n",
      " rmse : 9.487845537200716\n",
      "2700 \n",
      " cost : 91.62858418623476 \n",
      " rmse : 9.495428421817506\n",
      "2800 \n",
      " cost : 91.6213235697212 \n",
      " rmse : 9.506272682289572\n",
      "2900 \n",
      " cost : 91.60856493356212 \n",
      " rmse : 9.50224768451095\n",
      "3000 \n",
      " cost : 91.59849594679582 \n",
      " rmse : 9.505861775683451\n",
      "3100 \n",
      " cost : 91.58936633193437 \n",
      " rmse : 9.511004487438687\n",
      "3200 \n",
      " cost : 91.59258989805845 \n",
      " rmse : 9.527453822604462\n",
      "3300 \n",
      " cost : 91.58396481970999 \n",
      " rmse : 9.51605046663502\n",
      "3400 \n",
      " cost : 91.57791157848679 \n",
      " rmse : 9.515458048472162\n",
      "3500 \n",
      " cost : 91.57221666831757 \n",
      " rmse : 9.518216059246651\n",
      "3600 \n",
      " cost : 91.56677459094469 \n",
      " rmse : 9.521832633342381\n",
      "3700 \n",
      " cost : 91.56156420213948 \n",
      " rmse : 9.525581092281374\n",
      "3800 \n",
      " cost : 91.55656359258249 \n",
      " rmse : 9.529243647317188\n",
      "3900 \n",
      " cost : 91.56379253227287 \n",
      " rmse : 9.53319915830827\n",
      "4000 \n",
      " cost : 91.55195197723714 \n",
      " rmse : 9.533951423871981\n",
      "4100 \n",
      " cost : 91.54795507097097 \n",
      " rmse : 9.532286899984902\n",
      "4200 \n",
      " cost : 91.54417187144789 \n",
      " rmse : 9.533980508978495\n",
      "4300 \n",
      " cost : 91.54051391866432 \n",
      " rmse : 9.536182445761018\n",
      "4400 \n",
      " cost : 91.61850404458265 \n",
      " rmse : 9.52467359381223\n",
      "4500 \n",
      " cost : 91.53421011168099 \n",
      " rmse : 9.54050222665416\n",
      "4600 \n",
      " cost : 91.53267113684085 \n",
      " rmse : 9.546412781113972\n",
      "4700 \n",
      " cost : 91.53814092353359 \n",
      " rmse : 9.539682777705824\n",
      "4800 \n",
      " cost : 91.52585016136585 \n",
      " rmse : 9.542626185093097\n",
      "4900 \n",
      " cost : 91.54570306644305 \n",
      " rmse : 9.543808838335593\n",
      "5000 \n",
      " cost : 91.5202307504666 \n",
      " rmse : 9.543416612906421\n",
      "***** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate= 0.01)\n",
    "\n",
    "print('***** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    optimizer.minimize(loss=cost, var_list=[w1, b1, w2, b2, w3, b3])\n",
    "    if step % 100 == 0:\n",
    "        print('%04d' % step,'\\n', 'cost :', cost().numpy(),'\\n','rmse :',get_rmse(y_train, pred(x_train)))\n",
    "print('***** Learning Finished!!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(400, 1), dtype=float64, numpy=\narray([[21.58638832],\n       [22.62446864],\n       [22.83463194],\n       [22.18942512],\n       [22.28667731],\n       [22.42333882],\n       [22.9674424 ],\n       [22.48536917],\n       [22.62832914],\n       [22.75715855],\n       [22.61621151],\n       [22.48080785],\n       [22.65071976],\n       [22.76485501],\n       [22.98232563],\n       [22.23906243],\n       [22.44438194],\n       [22.59310378],\n       [23.52553864],\n       [22.51374734],\n       [22.47006327],\n       [22.57381909],\n       [22.62469156],\n       [22.65780325],\n       [22.62686697],\n       [22.71497482],\n       [22.31346397],\n       [21.91745235],\n       [22.56126919],\n       [22.62102317],\n       [22.63443641],\n       [22.35553785],\n       [22.63773185],\n       [22.75706464],\n       [21.28642406],\n       [22.66320534],\n       [22.41581137],\n       [23.0493569 ],\n       [22.60709574],\n       [23.02012186],\n       [23.08774943],\n       [22.65172357],\n       [23.11077718],\n       [23.2493722 ],\n       [21.69284288],\n       [22.95693632],\n       [22.38524853],\n       [22.65147466],\n       [22.61238962],\n       [22.0642199 ],\n       [22.88611859],\n       [23.38634416],\n       [22.64448224],\n       [22.63261226],\n       [22.6133661 ],\n       [22.57073744],\n       [23.06564832],\n       [22.25548547],\n       [23.22018232],\n       [22.38852147],\n       [22.82419473],\n       [22.65941258],\n       [22.78379645],\n       [23.46810593],\n       [23.04744181],\n       [22.63854852],\n       [22.57755084],\n       [21.90402611],\n       [22.61180337],\n       [21.95039757],\n       [22.62441298],\n       [22.63494173],\n       [22.43563499],\n       [22.63681686],\n       [22.82500828],\n       [22.59346455],\n       [22.5465722 ],\n       [22.90834537],\n       [22.4354654 ],\n       [23.46648821],\n       [22.19423049],\n       [23.39677417],\n       [22.12968234],\n       [22.67239065],\n       [22.54218897],\n       [22.83632998],\n       [22.56257587],\n       [22.61962066],\n       [22.41312631],\n       [22.63702769],\n       [22.35934083],\n       [22.63092443],\n       [22.57898874],\n       [22.61241331],\n       [22.67294415],\n       [22.76190049],\n       [22.27623352],\n       [22.48666548],\n       [22.73252829],\n       [23.76625883],\n       [22.36570063],\n       [22.88708604],\n       [22.56343349],\n       [23.20786725],\n       [23.50387238],\n       [22.48400892],\n       [22.56958959],\n       [22.59551195],\n       [22.80377108],\n       [22.53932569],\n       [21.83753317],\n       [22.5130309 ],\n       [22.14549357],\n       [22.6400433 ],\n       [22.49523961],\n       [22.93943952],\n       [22.21368881],\n       [22.42052831],\n       [22.61316266],\n       [22.71787087],\n       [22.31124597],\n       [22.56337937],\n       [22.5962747 ],\n       [22.32855395],\n       [22.46659084],\n       [23.32436889],\n       [22.81882964],\n       [22.11218663],\n       [22.31857058],\n       [22.85264052],\n       [22.72583105],\n       [22.81629201],\n       [22.62368934],\n       [22.62508322],\n       [22.6181867 ],\n       [22.78554598],\n       [22.63674373],\n       [23.19052298],\n       [23.130875  ],\n       [22.88325073],\n       [22.27989283],\n       [22.44269899],\n       [22.97074619],\n       [23.07465571],\n       [22.61709426],\n       [22.41908078],\n       [22.44588352],\n       [22.73527406],\n       [22.45416914],\n       [23.17112252],\n       [22.62064562],\n       [22.62460719],\n       [22.41239153],\n       [23.47926973],\n       [22.87716541],\n       [22.93869098],\n       [22.63195985],\n       [22.88002194],\n       [23.27723991],\n       [22.61178414],\n       [23.10374137],\n       [22.55977827],\n       [22.6804874 ],\n       [22.33855   ],\n       [22.531943  ],\n       [22.31965941],\n       [22.20925009],\n       [22.74443369],\n       [22.47561404],\n       [22.5770481 ],\n       [22.6271109 ],\n       [22.62828966],\n       [22.51910142],\n       [22.89922889],\n       [22.62851619],\n       [22.61784942],\n       [22.96009107],\n       [22.22549775],\n       [22.94746726],\n       [22.6341406 ],\n       [22.24710186],\n       [22.61744737],\n       [22.54783929],\n       [22.76359189],\n       [22.62911853],\n       [22.61670005],\n       [22.63997988],\n       [22.95490117],\n       [22.58490776],\n       [22.64419074],\n       [22.67075175],\n       [22.83360107],\n       [22.77529541],\n       [22.59289879],\n       [22.75363594],\n       [22.46263396],\n       [23.07207825],\n       [22.82351632],\n       [22.29493912],\n       [22.54788327],\n       [22.52888397],\n       [22.91976738],\n       [22.64538734],\n       [22.54103612],\n       [22.29457221],\n       [23.27713139],\n       [22.79874371],\n       [22.84946006],\n       [22.73297108],\n       [22.6248702 ],\n       [23.49225596],\n       [22.39842446],\n       [22.53023684],\n       [22.43283829],\n       [22.68742898],\n       [22.61100113],\n       [22.62102849],\n       [22.32258232],\n       [22.65326544],\n       [22.60676076],\n       [23.92520206],\n       [22.60790899],\n       [22.62306685],\n       [22.55451416],\n       [22.49351079],\n       [22.53440936],\n       [22.72466558],\n       [22.49266041],\n       [22.69214184],\n       [22.79725398],\n       [22.00085653],\n       [22.25230332],\n       [22.52042744],\n       [22.70847034],\n       [22.58189524],\n       [21.47459155],\n       [22.23079567],\n       [22.63284911],\n       [22.33604788],\n       [22.28691258],\n       [22.59380316],\n       [22.67660511],\n       [22.41211527],\n       [22.64513197],\n       [22.59773232],\n       [21.99602632],\n       [22.75457381],\n       [22.74561442],\n       [22.99620934],\n       [22.3172582 ],\n       [21.96847224],\n       [22.58403761],\n       [22.32856606],\n       [22.47241087],\n       [22.63183684],\n       [22.541614  ],\n       [22.05496436],\n       [22.51161925],\n       [22.56318032],\n       [22.13830723],\n       [22.39042729],\n       [22.58752441],\n       [22.57740324],\n       [23.01567249],\n       [22.55516707],\n       [22.66888113],\n       [22.63632909],\n       [22.16531086],\n       [22.53127422],\n       [22.65341934],\n       [22.71709681],\n       [22.55225926],\n       [22.35819384],\n       [22.85671893],\n       [22.24333054],\n       [22.69226008],\n       [22.67241227],\n       [22.30445278],\n       [22.64203083],\n       [22.45145784],\n       [22.68625984],\n       [22.75980072],\n       [22.60029281],\n       [22.0123392 ],\n       [22.64026281],\n       [22.2565758 ],\n       [22.27965489],\n       [23.02172561],\n       [22.37468024],\n       [23.40566055],\n       [21.96310367],\n       [23.19465933],\n       [22.48745381],\n       [22.61995966],\n       [22.41992122],\n       [22.58824868],\n       [22.94750222],\n       [22.96510752],\n       [22.25813586],\n       [22.63759594],\n       [22.57826944],\n       [22.6829633 ],\n       [22.63201273],\n       [21.99625721],\n       [22.27373483],\n       [22.50121327],\n       [21.88414533],\n       [22.62228514],\n       [21.59439366],\n       [23.01197228],\n       [21.86779513],\n       [22.62081284],\n       [23.04376443],\n       [22.79536032],\n       [22.63954569],\n       [22.64720678],\n       [23.27146557],\n       [22.9689854 ],\n       [22.55344072],\n       [22.21382223],\n       [22.6587527 ],\n       [23.0780657 ],\n       [23.19747685],\n       [22.44182996],\n       [22.16225803],\n       [22.58974201],\n       [22.869042  ],\n       [22.6153776 ],\n       [22.58266579],\n       [22.15437567],\n       [23.42834365],\n       [23.0475054 ],\n       [22.60801018],\n       [22.57559703],\n       [22.80478055],\n       [23.34050517],\n       [22.21637683],\n       [22.65255036],\n       [22.28936131],\n       [22.58779662],\n       [22.55203793],\n       [22.053852  ],\n       [22.43037127],\n       [22.29311725],\n       [22.6268754 ],\n       [22.42637766],\n       [23.40155631],\n       [22.60404821],\n       [22.53776057],\n       [23.15258955],\n       [22.56441163],\n       [22.62017438],\n       [22.59894685],\n       [22.63143402],\n       [22.62415108],\n       [22.10203078],\n       [22.37864731],\n       [22.44286125],\n       [22.46346863],\n       [22.75647702],\n       [22.62480817],\n       [22.86783879],\n       [22.31029632],\n       [22.2151237 ],\n       [22.61714525],\n       [22.56793284],\n       [22.36065458],\n       [22.61526597],\n       [22.26454644],\n       [22.81274694],\n       [22.31630166],\n       [22.59868182],\n       [23.07296149],\n       [22.35356823],\n       [23.64702454],\n       [22.70860816],\n       [22.66685977],\n       [22.51124948],\n       [22.53247765],\n       [22.5467392 ],\n       [22.78163908],\n       [22.71704391],\n       [22.32328245],\n       [22.79569586],\n       [23.17568043],\n       [22.60898837],\n       [22.54749723],\n       [22.61322792],\n       [22.27757937],\n       [22.62562785],\n       [22.15681631],\n       [22.73535124],\n       [22.58068707],\n       [22.35945751],\n       [23.05123581],\n       [22.79923346],\n       [22.43240101],\n       [22.44333122],\n       [22.66310145],\n       [22.5581021 ]])>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis(x_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[2] Caesarian 제왕절개 Logistic Regression 모델을 텐서플로를 사용하여 3층으로신경망을 구현하여 Accuracy를 구하여 단층 신경망과 비교하여 보세요\n",
    "    단층일 때보다 정확도가 높아지도록 구현한다"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "     0  1  2  3  4  5\n0   22  1  0  2  0  0\n1   26  2  0  1  0  1\n2   26  2  1  1  0  0\n3   28  1  0  2  0  0\n4   22  2  0  1  0  1\n..  .. .. .. .. .. ..\n75  27  2  1  1  0  0\n76  33  4  0  1  0  1\n77  29  2  1  2  0  1\n78  25  1  2  0  0  1\n79  24  2  2  1  0  0\n\n[80 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>27</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>33</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>29</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>25</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df2 = pd.read_csv('C:/Users/HOME/PycharmProjects/ai/.ipynb_checkpoints/data_set/caesarian.csv', header= None)\n",
    "df2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[22,  1,  0,  2,  0],\n        [26,  2,  0,  1,  0],\n        [26,  2,  1,  1,  0],\n        [28,  1,  0,  2,  0],\n        [22,  2,  0,  1,  0],\n        [26,  1,  1,  0,  0],\n        [27,  2,  0,  1,  0],\n        [32,  3,  0,  1,  0],\n        [28,  2,  0,  1,  0],\n        [27,  1,  1,  1,  0],\n        [36,  1,  0,  1,  0],\n        [33,  1,  1,  0,  0],\n        [23,  1,  1,  1,  0],\n        [20,  1,  0,  1,  1],\n        [29,  1,  2,  0,  1],\n        [25,  1,  2,  0,  0],\n        [25,  1,  0,  1,  0],\n        [20,  1,  2,  2,  0],\n        [37,  3,  0,  1,  1],\n        [24,  1,  2,  0,  1],\n        [26,  1,  1,  1,  0],\n        [33,  2,  0,  0,  1],\n        [25,  1,  1,  2,  0],\n        [27,  1,  0,  0,  1],\n        [20,  1,  0,  2,  1],\n        [18,  1,  0,  1,  0],\n        [18,  1,  1,  2,  1],\n        [30,  1,  0,  1,  0],\n        [32,  1,  0,  2,  1],\n        [26,  2,  1,  1,  1],\n        [25,  1,  0,  0,  0],\n        [40,  1,  0,  1,  1],\n        [32,  2,  0,  2,  1],\n        [27,  2,  0,  1,  1],\n        [26,  2,  2,  1,  0],\n        [28,  3,  0,  2,  0],\n        [33,  1,  1,  1,  0],\n        [31,  2,  2,  1,  0],\n        [31,  1,  0,  1,  0],\n        [26,  1,  2,  0,  1],\n        [27,  1,  0,  2,  1],\n        [19,  1,  0,  1,  0],\n        [36,  1,  1,  2,  0],\n        [22,  1,  0,  1,  0],\n        [36,  4,  0,  2,  1],\n        [28,  3,  0,  1,  1],\n        [26,  1,  0,  1,  0],\n        [32,  2,  0,  2,  1],\n        [26,  2,  2,  1,  0],\n        [29,  2,  0,  0,  1],\n        [33,  3,  2,  1,  1],\n        [21,  2,  1,  0,  1],\n        [30,  3,  2,  2,  0],\n        [35,  1,  1,  0,  0],\n        [29,  2,  0,  1,  1],\n        [25,  2,  0,  1,  0]], dtype=int64),\n array([[1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [0, 1],\n        [1, 0]], dtype=uint8))"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = df2.iloc[:56,:-1].to_numpy()\n",
    "y_train = pd.get_dummies(df2.iloc[:56, -1]).to_numpy()\n",
    "x_train, y_train\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[32,  3,  1,  0,  1],\n       [21,  1,  0,  0,  0],\n       [26,  1,  0,  2,  0],\n       [30,  2,  1,  2,  1],\n       [22,  1,  2,  2,  0],\n       [19,  1,  0,  1,  0],\n       [32,  2,  0,  0,  0],\n       [32,  2,  0,  1,  1],\n       [31,  1,  2,  2,  1],\n       [35,  2,  0,  1,  0],\n       [28,  3,  0,  1,  0],\n       [29,  2,  0,  1,  1],\n       [25,  1,  0,  0,  0],\n       [27,  2,  2,  0,  0],\n       [17,  1,  0,  0,  0],\n       [29,  1,  2,  0,  1],\n       [28,  2,  0,  1,  0],\n       [32,  3,  0,  1,  1],\n       [38,  3,  2,  2,  1],\n       [27,  2,  1,  1,  0],\n       [33,  4,  0,  1,  0],\n       [29,  2,  1,  2,  0],\n       [25,  1,  2,  0,  0],\n       [24,  2,  2,  1,  0]], dtype=int64)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = df2.iloc[56:,:-1].to_numpy()\n",
    "y_test = df2.iloc[56:, -1].to_numpy()\n",
    "x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def layer1(X):\n",
    "    return  tf.nn.relu(tf.matmul(tf.Variable(X, dtype=tf.float64),w1) + b1 )\n",
    "\n",
    "def layer2(X):\n",
    "    return  tf.nn.relu(tf.matmul(layer1(X),w2) + b2 )\n",
    "\n",
    "def hypothesis(X):\n",
    "    return  tf.sigmoid(tf.matmul(layer2(X),w3) + b3 )\n",
    "\n",
    "def cost():\n",
    "    h = hypothesis(x_train)\n",
    "    c = -tf.reduce_mean(y_train*tf.math.log(h) +\n",
    "                           (1-y_train)*tf.math.log(1-h))\n",
    "    return c\n",
    "\n",
    "def pred(x):\n",
    "    return hypothesis(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.normal([5,3], dtype=tf.float64, name='weight1'))\n",
    "b1 = tf.Variable(tf.random.normal([3], dtype=tf.float64, name='bias1'))\n",
    "\n",
    "w2 = tf.Variable(tf.random.normal([3,2], dtype=tf.float64, name='weight2'))\n",
    "b2 = tf.Variable(tf.random.normal([2], dtype=tf.float64, name='bias2'))\n",
    "\n",
    "w3 = tf.Variable(tf.random.normal([2,1], dtype=tf.float64, name='weight3'))\n",
    "b3 = tf.Variable(tf.random.normal([1], dtype=tf.float64, name='bias3'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 \n",
      " cost : tf.Tensor(2.035669787908727, shape=(), dtype=float64) \n",
      " rmse : 0.6666495122285916\n",
      "0100 \n",
      " cost : tf.Tensor(0.6931480577775287, shape=(), dtype=float64) \n",
      " rmse : 0.5000477416987569\n",
      "0200 \n",
      " cost : tf.Tensor(0.6931471806143242, shape=(), dtype=float64) \n",
      " rmse : 0.5000003724811765\n",
      "0300 \n",
      " cost : tf.Tensor(0.6931471805599473, shape=(), dtype=float64) \n",
      " rmse : 0.4999999977007028\n",
      "0400 \n",
      " cost : tf.Tensor(0.6931471805599452, shape=(), dtype=float64) \n",
      " rmse : 0.4999999999891032\n",
      "0500 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.49999999999997446\n",
      "0600 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "0700 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "0800 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "0900 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1000 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1100 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1200 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1300 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1400 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1500 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1600 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1700 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1800 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "1900 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2000 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2100 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2200 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2300 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2400 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2500 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2600 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2700 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2800 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "2900 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3000 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3100 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3200 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3300 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3400 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3500 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3600 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3700 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3800 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "3900 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4000 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4100 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4200 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4300 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4400 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4500 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4600 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4700 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4800 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "4900 \n",
      " cost : tf.Tensor(0.6931471805599454, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "5000 \n",
      " cost : tf.Tensor(0.6931471805599453, shape=(), dtype=float64) \n",
      " rmse : 0.5\n",
      "***** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate= 0.10)\n",
    "\n",
    "print('***** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    optimizer.minimize(loss=cost, var_list=[w1, b1, w2, b2, w3, b3])\n",
    "    if step % 100 == 0:\n",
    "        print('%04d' % step,'\\n', 'cost :', cost(),'\\n','rmse :',get_rmse(y_train, pred(x_train)))\n",
    "print('***** Learning Finished!!')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
